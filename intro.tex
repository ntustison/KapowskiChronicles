\section{Introduction}

% Neuroscientific investigations into cortical morphological
% changes/differences have illuminated interesting correlations with
% normal and pathological neurodevelopment in  addition to cognitive function.
Historically rooted in the meticulous work of von Economo \citep{economo2008},
imaging-based structural analysis of the brain plays a fundamental role
in identifying the relationship between cortical morphology, disease and cognition.
Discriminative quantitative cortical measures have been demonstrated in conditional 
abnormalities such as 
Huntington's disease \citep{rosas2002,rosas2005,selemon2004}, 
schizophrenia \citep{nesvag2008}, bipolar disorder \cite{lyoo2006}, Alzheimer's disease and frontotemporal
dementia \citep{du2007,dickerson2009}, Parkinson's disease \citep{jubault2011}, Williams syndrome \citep{thompson2005},
multiple sclerosis \citep{ramasamy2009}, autism \citep{chung2005,hardan2006},
migraines \citep{dasilva2007}, chronic smoking \citep{kuhn2010}, alcoholism \citep{fortier2011},
cocaine addiction \citep{makris2008}, Tourette syndrome in children \citep{sowell2008},
scoliosis in
female adolescents \citep{wang2012}, obsessive compulsive
disorder \citep{shin2007}, ADHD \citep{almeida-montes2012}, obesity \citep{raji2010}, and heritable \citep{peterson2009}
and elderly \citep{ballmaier2004} depression.  Cortical thickness also
varies normally as a function of age \citep{kochunov2011},
gender \citep{luders2006a}, untreated
male-to-female transsexuality \citep{luders2012},  handedness
\citep{luders2006,amunts2007}, intelligence \citep{shaw2006}, athletic
ability \citep{wei2011}, musical ability \citep{bermudez2009,foster2010}, 
tendency toward criminality \citep{raine2011}, and Tetris-playing
ability in female adolescents \citep{haier2009}.  Additionally,
recent studies demonstrate structural 
connectivity relationships using cortical thickness measures
\citep{worsley2005,lerch2006,he2007,chen2008}.
Although these findings
are subject to debate and interpretation \citep{gernsbacher2007}, 
the availability of quantitative
computational methods for extracting such information
has proven invaluable for developing and refining fundamental 
neuroscience hypotheses.

Computational methods for analyzing the cortex may be 
broadly characterized as surface mesh-based or volumetric \citep{scott2009,clarkson2011}.  Representative of the former is the
Freesurfer%
\footnote{
http://surfer.nmr.mgh.harvard.edu/
}
cortical modeling software package \citep{dale1999,fischl1999,fischl2000,fischl2002,fischl2004}
which owes its popularity to public availability, excellent documentation, 
good performance, and  integration with other toolkits, such as the extensive FMRIB software 
library (FSL) \citep{smith2004}.  Similar to other surface
approaches (e.g. \cite{davatzikos1996,magnotta1999,macdonald2000,kim2005}), the pial
and white matter surfaces from individual subject MR data are modeled with polygonal meshes  
which are then used to determine local cortical thickness values based on a specified correspondence between 
the surface models.

Image volumetric (or meshless) techniques vary both in algorithmic terms as well as
the underlying definition of cortical thickness.  An early, foundational technique is the 
method of \cite{jones2000} in which the inner and outer surface geometry is used to determine the
solution to Laplace's equation where thickness is measured by integrating along the 
tangents of the resulting field lines spanning the boundary surfaces.  Subsequent contributions
improved upon the original formulation.  For example, in \cite{yezzi2003}, an Eulerian PDE approach
was proposed to facilitate the computation of correspondence paths.  Extending the surface-based
work of \cite{macdonald2000}, the hybrid approach of
\cite{kim2005} uses the discrete Laplacian field to deform the white matter surface mesh towards the 
pial surface.    Although the Laplacian-based approach has several advantages
including generally lower computational times and 
non-crossing correspondence paths, direct correlative assessments with histology
are potentially problematic as the quantified distances 
are not necessarily Euclidean.  Other volumetric algorithms employ coupled
level sets \citep{zeng1999}, model-free intelligent search strategies either normal to 
the gray-white matter interface \citep{scott2009} or using a min-max rule \citep{clement-vachet2011}.
Most relevant to this work is the DiReCT (Diffeomorphic Registration-based 
Cortical Thickness) algorithm proposed in \cite{das2009} where generated
diffeomorphic mappings between the 
white and pial matter surfaces are used to propagate thickness values 
through the cortical gray matter.  A unique benefit of DiReCT is that it
naturally estimates the boundaries of buried sulci by employing a
diffeomorphic constraint on the probabilistic estimate of the gray
matter and cerebrospinal fluid interface.  

Despite the variety of techniques for estimating cortical thickness
from imaging data (of which
only a fraction are cited), several common preprocessing components
may be identified.
The most fundamental of these include inhomogeneity correction, skull stripping, and $n$-tissue segmentation 
for differentiating the gray and white matter.  For statistical analysis 
across large populations, construction of population-specific unbiased templates
is also potentially beneficial \citep{evans2012}.
In addition, intermediate steps might include a crucial registration component (e.g. 
propagating template-based tissue priors for improved segmentation).

  The requisite
additional components apart from the actual cortical thickness estimation, 
coupled with the general lack of availability of published
algorithms \citep{kovacevic2006}, inhibits performing studies by external researchers 
and makes comparative evaluations difficult.  For example, one recent evaluation 
study \citep{clarkson2011} compared
Freesurfer (a surface-based method) with two volumetric methods \citep{jones2000,das2009}.
Whereas the entire Freesurfer processing pipeline has been made publicly available, 
refined by the original authors and other contributors, and described in great detail 
(specifically in terms of suggested parameters); both volumetric methods were 
implemented solely by the authors of the evaluation (not the actual algorithm developers) using 
unspecified parameters making the comparisons less than
ideal (see \cite{tustison2013} for further discussion concerning the issue of instrumentation bias in
the use and evaluation of software). Further complicating comparisons is distinct processing domains between
volumetric and surface-based techniques and the potential for the introduction
of bias \citep{klein2010}.

In this work, we describe our Insight Toolkit (ITK)-based cortical thickness pipeline
which is freely available as part of the Advanced Normalization Tools
(ANTs) software package.  This includes all the necessary preprocessing steps consisting
of well-vetted previously published algorithms for bias correction \citep{tustison2010},
brain extraction \citep{avants2010a}, $n$-tissue segmentation \citep{avants2011a},
template construction \citep{avants2010}, and image normalization \citep{avants2011}.
We also describe improvements made to the original DiReCT algorithm \citep{das2009}.
Equally as important, we provide explicit coordination between
these pipeline components within a set of well-documented shell scripts which 
is also available in the ANTs repository where parameters have been tuned
by ANTs developers, viz., N.T. and B.A,
and provide good performance across a number of data sets.
Furthermore, we post all the derived image data and processing scripts 
using the online tools figshare%
\footnote{
http://www.figshare.com
}
 and github%
\footnote{
http://www.github.com
}, respectively.
The
availability of both the code and data permits
the set of results described in this work to be fully reproducible.  This
permits other researchers to contrast their own results against
this baseline processing and to adapt the given volumetric pipeline for measuring
cortical thickness with their own datasets.


%This might explain why there have been few evaluation studies.  For example, in
%comparing volumetric and surface-based methods, \cite{clarkson2011} use the
%Freesurfer implementation but rely on their own implementations of published 
%literature which might not be an unbiased evaluation particularly given the 
%complexity of the underlying registration algorithmic work in \cite{das2009}.
%Based solely on the number of citations in the literature,
%the Freesurfer%
%\footnote{
%http://surfer.nmr.mgh.harvard.edu/
%} 
%cortical modeling software package is perhaps the most ubiquitous 
%with several publications detailing various stages of development and
%methodology \citep{dale1999,fischl1999,fischl2000,fischl2002,fischl2004}.  
%Public availability, excellent documentation, good performance, and 
%integration with other toolkits, such as the extensive FMRIB software 
%library (FSL) \cite{smith2004},
%have contributed to its popularity.  



%Freesurfer's individual brain processing pipeline begins with segmentation
%and surface modeling of the gray white matter interface.  Gradients 


%\begin{table*}
%\caption{Component-wise breakdown of reported cortical thickness methods reported in the literature.}
%\begin{tabular*}{0.95\textwidth}{@{\extracolsep{\fill}} c c c c c p{4.5cm} }
%\hline
%{\bf Algorithm} & {\bf V/S} & {\bf Bias correction} & {\bf Brain extraction} & {\bf Segmentation} & \multicolumn{1}{c}{\bf Additional notes} \\
%\hline
%BRAINSURF \cite{magnotta1999} & S & {} & none & Discriminate analysis \cite{harris1999} & {} \\
%ASP \cite{macdonald2000} & S & {} & N3$^\dagger$ \cite{Sled1998} & Neural-nets \cite{ozkan1993} & {}\\
%Laplace \cite{Jones2000} & V & {} & none & simple thresholding & {} \\
%CLASP \cite{kim2005} & V/S & {} & N3$^\dagger$ \cite{Sled1998} & K-NN \cite{cocosco2003} & { Partial volume classification of GM/CSF \cite{Choi1991} is used to facilitate reconstructing the pial surface. }\\
%\cite{hutton2005} & V & {} & {} & {} & {} \\
%DiReCT \cite{das2009} & V & {} & none & FAST$^\dagger$ \cite{zhang2001} & {} \\
%\cite{scott2009} & V & {} & none & E-M Bayes \cite{pokric2001}  & {} \\
%\cite{acosta2009} & V & {} & \cite{van-leemput1999a} & E-M Bayes. \cite{van-leemput1999} & {}\\
%\cite{lerch2005}$^{BrainVisa}$ & {} & {} & {} & {} & {} \\
%CRUISE\cite{han2004,tosun2006} & S& {}  & TOADS$^\dagger$ \cite{bazin2007} & {} \\
%
%CLADA\cite{nakamura2011} & S& {}  & PABIC$^\dagger$ \cite{styner2000} & \cite{nakamura2009} & {longitudinal analysis}\\
%%SIENA$^\dagger$\cite{smith2002} & {} & {} & FAST$^\dagger$ \cite{zhang2001} & {longitudinal analysis}\\
%\hline
%\end{tabular*}
%\end{table*}
%
%Open source packages:
%\begin{itemize}
%\item (http://www.ncbi.nlm.nih.gov/pubmed/15957597) TINA - be sure to read the reviews which aren't very good.
%\item (http://www.nitrc.org/projects/arctic/ %http://www.na-mic.org/Wiki/index.php/UNC_ARCTIC_Tutorial) 
%ARCTIC (Automatic Regional Cortical ThICkness)
%\item Brain Voyager (Goebel?)
%\item TOADS-CRUISE (Tosun et al.) http://www.nitrc.org/projects/toads-cruise/
%\item GAMBIT
%\item http://www.bic.mni.mcgill.ca/thickness\_population\_simulation/ \cite{lerch2005}
%\item Be sure to email Vincent Magnotta to see if they have open source tools in Brains for estimation of cortical thickness
%\end{itemize}
